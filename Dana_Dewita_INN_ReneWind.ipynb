{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DanaDewita/Documents/blob/master/Dana_Dewita_INN_ReneWind.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JdME3J172AN-"
      },
      "source": [
        "# **Actionable Insights and Recommendations**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "64a66d7f"
      },
      "source": [
        "\n",
        "## üìä Business Insights and Recommendations (Post-Tuning + SHAP)\n",
        "\n",
        "### ‚úÖ What We Did ‚Äî and Why It Matters\n",
        "- üß† We trained a model to predict where things might go wrong based on hidden feilds, these may be late payments, missed renewals, or customer churn.\n",
        "- üìà We fine-tuned it using Keras Tuner to make sure it catches risks earlier and more accurately.\n",
        "- üîç We added SHAP ‚Äî a transparency tool that explains **why** the model flags something.\n",
        "- ‚úÖ This combination builds trust and helps teams act with confidence.\n",
        "\n",
        "### üîç What Changed with Tuning and SHAP\n",
        "- üéØ Tuning helped the model become more sensitive to real risk ‚Äî especially ones we used to miss (false negatives).\n",
        "- üí¨ SHAP shows exactly which factors (e.g., contract type, payment behavior) triggered the prediction.\n",
        "- üìâ Together, they reduce both surprises and unnecessary escalations.\n",
        "\n",
        "### üí° What We Learned (Insights)\n",
        "1. A small number of features drive most of the model‚Äôs decisions.\n",
        "2. SHAP allows us to **explain individual predictions** ‚Äî critical for trust and action.\n",
        "3. These explanations highlight **what matters most to improve**: data quality, customer behavior, or internal processes.\n",
        "\n",
        "### üß≠ What We Recommend (Business Actions)\n",
        "| Recommendation | Who Benefits | Why It Matters |\n",
        "|----------------|--------------|----------------|\n",
        "| Use model + SHAP to flag accounts needing manual review | Ops, Risk Teams | Focus on the riskiest customers or contracts |\n",
        "| Prioritize follow-ups using SHAP feature impact | Customer Success | Act before issues escalate |\n",
        "| Share SHAP findings in weekly triage or dashboard | Data/IT + CX | Helps cross-teams align on what drives risk |\n",
        "| Rerun tuning and SHAP review every quarter | ML/IT Governance | Keeps model accurate over time |\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9EaJ8AGwpM-2"
      },
      "source": [
        "# **Problem Statement**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x3-QehJxbp0t"
      },
      "source": [
        "## Business Context"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7DoY1AHZfMF1"
      },
      "source": [
        "Renewable energy sources play an increasingly important role in the global energy mix, as the effort to reduce the environmental impact of energy production increases.\n",
        "\n",
        "Out of all the renewable energy alternatives, wind energy is one of the most developed technologies worldwide. The U.S Department of Energy has put together a guide to achieving operational efficiency using predictive maintenance practices.\n",
        "\n",
        "Predictive maintenance uses sensor information and analysis methods to measure and predict degradation and future component capability. The idea behind predictive maintenance is that failure patterns are predictable and if component failure can be predicted accurately and the component is replaced before it fails, the costs of operation and maintenance will be much lower.\n",
        "\n",
        "The sensors fitted across different machines involved in the process of energy generation collect data related to various environmental factors (temperature, humidity, wind speed, etc.) and additional features related to various parts of the wind turbine (gearbox, tower, blades, break, etc.)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4p9zp6bHfPOY"
      },
      "source": [
        "## Objective"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eu7YIw90fRdW"
      },
      "source": [
        "‚ÄúReneWind‚Äù is a company working on improving the machinery/processes involved in the production of wind energy using machine learning and has collected data of generator failure of wind turbines using sensors. They have shared a ciphered version of the data, as the data collected through sensors is confidential (the type of data collected varies with companies). Data has 40 predictors, 20000 observations in the training set and 5000 in the test set.\n",
        "\n",
        "The objective is to build various classification models, tune them, and find the best one that will help identify failures so that the generators could be repaired before failing/breaking to reduce the overall maintenance cost.\n",
        "The nature of predictions made by the classification model will translate as follows:\n",
        "\n",
        "- True positives (TP) are failures correctly predicted by the model. These will result in repairing costs.\n",
        "- False negatives (FN) are real failures where there is no detection by the model. These will result in replacement costs.\n",
        "- False positives (FP) are detections where there is no failure. These will result in inspection costs.\n",
        "\n",
        "It is given that the cost of repairing a generator is much less than the cost of replacing it, and the cost of inspection is less than the cost of repair.\n",
        "\n",
        "‚Äú1‚Äù in the target variables should be considered as ‚Äúfailure‚Äù and ‚Äú0‚Äù represents ‚ÄúNo failure‚Äù."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XPeJAS5bfUOj"
      },
      "source": [
        "## Data Description"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P4ns9KiyfVsW"
      },
      "source": [
        "The data provided is a transformed version of the original data which was collected using sensors.\n",
        "\n",
        "- Train.csv - To be used for training and tuning of models.\n",
        "- Test.csv - To be used only for testing the performance of the final best model.\n",
        "\n",
        "Both the datasets consist of 40 predictor variables and 1 target variable."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wesRJFbQrTs7"
      },
      "source": [
        "# **Please read the instructions carefully before starting the project.**\n",
        "This is a commented Jupyter IPython Notebook file in which all the instructions and tasks to be performed are mentioned.\n",
        "* Blanks '_______' are provided in the notebook that\n",
        "needs to be filled with an appropriate code to get the correct result. With every '_______' blank, there is a comment that briefly describes what needs to be filled in the blank space.\n",
        "* Identify the task to be performed correctly, and only then proceed to write the required code.\n",
        "* Fill the code wherever asked by the commented lines like \"# write your code here\" or \"# complete the code\". Running incomplete code may throw error.\n",
        "* Please run the codes in a sequential manner from the beginning to avoid any unnecessary errors.\n",
        "* Add the results/observations (wherever mentioned) derived from the analysis in the presentation and submit the same."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v_-uuGqH-qTt"
      },
      "source": [
        "# **Installing and Importing the necessary libraries**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ob7CHevooE2s"
      },
      "outputs": [],
      "source": [
        "# Installing the libraries with the specified version\n",
        "!pip install --no-deps tensorflow==2.18.0 scikit-learn==1.3.2 matplotlib===3.8.3 seaborn==0.13.2 numpy==1.26.4 pandas==2.2.2 -q --no-warn-script-location"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "03ebf93b"
      },
      "outputs": [],
      "source": [
        "!pip install shap -q"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QKE5LWqhvChF"
      },
      "source": [
        "**Note**:\n",
        "- After running the above cell, kindly restart the runtime (for Google Colab) or notebook kernel (for Jupyter Notebook), and run all cells sequentially from the next cell.\n",
        "- On executing the above line of code, you might see a warning regarding package dependencies. This error message can be ignored as the above code ensures that all necessary libraries and their dependencies are maintained to successfully execute the code in ***this notebook***."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dbiVVX6hbp0v"
      },
      "outputs": [],
      "source": [
        "# Library for data manipulation and analysis.\n",
        "import pandas as pd\n",
        "# Fundamental package for scientific computing.\n",
        "import numpy as np\n",
        "#splitting datasets into training and testing sets.\n",
        "from sklearn.model_selection import train_test_split\n",
        "#Imports tools for data preprocessing including label encoding, one-hot encoding, and standard scaling\n",
        "from sklearn.preprocessing import LabelEncoder, OneHotEncoder,StandardScaler\n",
        "#Imports a class for imputing missing values in datasets.\n",
        "from sklearn.impute import SimpleImputer\n",
        "#Imports the Matplotlib library for creating visualizations.\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "# Imports the Seaborn library for statistical data visualization.\n",
        "import seaborn as sns\n",
        "# Time related functions.\n",
        "import time\n",
        "#Imports functions for evaluating the performance of machine learning models\n",
        "from sklearn.metrics import confusion_matrix, f1_score,accuracy_score, recall_score, precision_score, classification_report\n",
        "#Imports metrics from\n",
        "from sklearn import metrics\n",
        "\n",
        "#Imports the tensorflow,keras and layers.\n",
        "import tensorflow\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.layers import Dense, Input, Dropout,BatchNormalization\n",
        "from tensorflow.keras import backend\n",
        "\n",
        "# to suppress unnecessary warnings\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f741d1d5"
      },
      "outputs": [],
      "source": [
        "import shap"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xxhpZv9y-qTw"
      },
      "source": [
        "# **Loading the Data**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D4QgYmaOkMEA"
      },
      "outputs": [],
      "source": [
        "# uncomment and run the following lines for Google Colab\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "39UgpBY3bp0y"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "df= pd.read_csv('/content/drive/MyDrive/Colab Notebooks/ReneWind_Maintenance/Train.csv') #Complete the code to import the training data\n",
        "df_test = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/ReneWind_Maintenance/Test.csv')    # Complete the code to import the test data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4cVx5kZHbp02"
      },
      "source": [
        "# **Data Overview**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W-5jUOgu-qTz"
      },
      "source": [
        "The initial steps to get an overview of any dataset is to:\n",
        "- observe the first few rows of the dataset, to check whether the dataset has been loaded properly or not\n",
        "- get information about the number of rows and columns in the dataset\n",
        "- find out the data types of the columns to ensure that data is stored in the preferred format and the value of each property is as expected.\n",
        "- check the statistical summary of the dataset to get an overview of the numerical columns of the data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KQi5ygTC-qT1"
      },
      "source": [
        "## Checking the shape of the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7Y8epHpjbp0z"
      },
      "outputs": [],
      "source": [
        "# Checking the number of rows and columns in the training data\n",
        "df.shape # Complete the code to print the shape of the train data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hA15qTjzbp01"
      },
      "outputs": [],
      "source": [
        "# Checking the number of rows and columns in the test data\n",
        "df_test.shape # Complete the code to print the shape of the test data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QTsYJZBubp02"
      },
      "outputs": [],
      "source": [
        "# let's create a copy of the training data\n",
        "data = df.copy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "69ImWbLXbp03"
      },
      "outputs": [],
      "source": [
        "# let's create a copy of the testing  data\n",
        "data_test = df_test.copy()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qlzqMR1K-qTz"
      },
      "source": [
        "## Displaying the first few rows of the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l9RnN7Twbp03"
      },
      "outputs": [],
      "source": [
        "# let's view the first 5 rows of the data\n",
        "data.head() # Complete the code to view the first five rows of the train data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "47OhpabimjMy"
      },
      "outputs": [],
      "source": [
        "#viewing first 5 rows of the test data\n",
        "data_test.head () # Complete the code to view the first five rows of the test data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5TcqcxbK-qT3"
      },
      "source": [
        "## Checking the data types of the columns in the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EXBYJoKkbp04"
      },
      "outputs": [],
      "source": [
        "# let's check the data types of the columns in the dataset\n",
        "data.info() # Complete the code to view the data types of the columns in the train data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LvN3_-Vdbp04"
      },
      "source": [
        "- Converting Target column to float"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6LwVuEgjmHxW"
      },
      "outputs": [],
      "source": [
        "data['Target'] = data['Target'].astype(float)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4MmheoOtnYZx"
      },
      "source": [
        "Now checking for test data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Utfm_wZsnR1s"
      },
      "outputs": [],
      "source": [
        "data_test.info() # Complete the code to view the data types of the columns in the test data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KyX9vFVFnk5t"
      },
      "source": [
        "Converting Target to float"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SfTJuArBnbIU"
      },
      "outputs": [],
      "source": [
        "data_test['Target'] = data_test['Target'].astype(float)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xNr4bWoM-qT5"
      },
      "source": [
        "## Checking for duplicate values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V0EmBHNmbp04"
      },
      "outputs": [],
      "source": [
        "# let's check for duplicate values in the data\n",
        "data.duplicated() # Complete the code to check for duplicate values in the train data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ch_TjRfF-qT5"
      },
      "source": [
        "## Checking for missing values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SlwFZm-Jbp05"
      },
      "outputs": [],
      "source": [
        "# let's check for missing values in the data\n",
        "data.isnull().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_Qx6zDRLmxg5"
      },
      "outputs": [],
      "source": [
        "data.dropna(inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "33bar6robp06"
      },
      "outputs": [],
      "source": [
        "# let's check for missing values in the test data\n",
        "data_test.isnull().sum() #"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4uOyFr0_l3ad"
      },
      "outputs": [],
      "source": [
        "data_test.dropna(inplace=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nUCorhch-qT4"
      },
      "source": [
        "## Statistical summary of the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J6lzvHKCbp06"
      },
      "outputs": [],
      "source": [
        "# let's view the statistical summary of the numerical columns in the data\n",
        "data.describe() # Complete the code to view the statistical summary of the train data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DhPuzWO7hmV8"
      },
      "source": [
        "# **Exploratory Data Analysis**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lv7Bs8aUbp07"
      },
      "source": [
        "## Univariate analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QIP4bI3Zbp07"
      },
      "outputs": [],
      "source": [
        "# function to plot a boxplot and a histogram along the same scale.\n",
        "\n",
        "\n",
        "def histogram_boxplot(data, feature, figsize=(12, 7), kde=False, bins=None):\n",
        "    \"\"\"\n",
        "    Boxplot and histogram combined\n",
        "\n",
        "    data: dataframe\n",
        "    feature: dataframe column\n",
        "    figsize: size of figure (default (12,7))\n",
        "    kde: whether to the show density curve (default False)\n",
        "    bins: number of bins for histogram (default None)\n",
        "    \"\"\"\n",
        "    f2, (ax_box2, ax_hist2) = plt.subplots(\n",
        "        nrows=2,  # Number of rows of the subplot grid= 2\n",
        "        sharex=True,  # x-axis will be shared among all subplots\n",
        "        gridspec_kw={\"height_ratios\": (0.25, 0.75)},\n",
        "        figsize=figsize,\n",
        "    )  # creating the 2 subplots\n",
        "    sns.boxplot(\n",
        "        data= data, x= feature, ax= ax_box2, showmeans=True, color=\"violet\"\n",
        "    )  # boxplot will be created and a star will indicate the mean value of the column\n",
        "    sns.histplot(\n",
        "        data= data, x= feature, kde=kde, ax= ax_hist2, bins=bins, palette=\"winter\"\n",
        "    ) if bins else sns.histplot(\n",
        "        data= data, x= feature, kde=kde, ax= ax_hist2\n",
        "    )  # For histogram\n",
        "    ax_hist2.axvline(\n",
        "        data[feature].mean(), color=\"green\", linestyle=\"--\"\n",
        "    )  # Add mean to the histogram\n",
        "    ax_hist2.axvline(\n",
        "        data[feature].median(), color=\"black\", linestyle=\"-\"\n",
        "    )  # Add median to the histogram"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_5N52iqsJBeb"
      },
      "source": [
        "### Variables V1 to V40"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8e1430ec"
      },
      "outputs": [],
      "source": [
        "for feature in data.columns:\n",
        "    histogram_boxplot(df, feature, figsize=(12, 7), kde=False, bins=None)\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jchAb6Ikbp08"
      },
      "outputs": [],
      "source": [
        "# For train data\n",
        "df[\"Target\"].value_counts(1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AnbouDSRJCpA"
      },
      "source": [
        "The output shows the distribution of the Target variable in your training data.\n",
        "\n",
        "Target 0: Represents the majority class, accounting for approximately 94.45% of the observations. This corresponds to \"No failure\".\n",
        "Target 1: Represents the minority class, accounting for approximately 5.55% of the observations. This corresponds to \"failure\".\n",
        "This indicates that your dataset is imbalanced, with significantly fewer instances of the \"failure\" class compared to the \"No failure\" class. This is an important observation for model building, as imbalanced datasets can affect the performance of classification models, particularly in predicting the minority class."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3CjaGOdvbp08"
      },
      "source": [
        "### Checking the distrubution of Target variable"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5rupsM4sbp08"
      },
      "outputs": [],
      "source": [
        "# For test data\n",
        "data_test[\"Target\"].value_counts(normalize=True) # Complete the code to display the proportion of the target variable in the test data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lRrYccqLJKXi"
      },
      "source": [
        "## Bivariate Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lk0t8FYwJMeL"
      },
      "source": [
        "### Correlation Check"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kL32Bf4rJO1F"
      },
      "outputs": [],
      "source": [
        "cols_list = df.select_dtypes(include=np.number).columns.tolist()\n",
        "cols_list.remove(\"Target\")\n",
        "\n",
        "plt.figure(figsize=(20, 20))\n",
        "sns.heatmap(\n",
        "    df[cols_list].corr(), annot=True, vmin=-1, vmax=1, fmt=\".2f\", cmap=\"Spectral\"\n",
        ")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s_vYgdsnDQ_L"
      },
      "outputs": [],
      "source": [
        "# Calculate the correlation matrix\n",
        "correlation_matrix = df[cols_list].corr()\n",
        "\n",
        "# Stack the correlation matrix to easily filter pairs\n",
        "stacked_corr = correlation_matrix.stack()\n",
        "\n",
        "# Filter for correlations above 0.7 or below -0.7 (excluding self-correlation)\n",
        "high_corr = stacked_corr[\n",
        "    (abs(stacked_corr) >= 0.7) & (stacked_corr != 1.0)\n",
        "]\n",
        "\n",
        "# Print the pairs of highly correlated variables\n",
        "print(\"Pairs of variables with correlation >= 0.7 or <= -0.7:\")\n",
        "print(high_corr)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bp8vC9MZbp09"
      },
      "source": [
        "# **Data Preprocessing**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CMTLLop3yDwk"
      },
      "source": [
        "## Data Preparation for Modeling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C-xgnTbGb4CY"
      },
      "outputs": [],
      "source": [
        "# Dividing train data into X and y\n",
        "X = data.drop(columns = [\"Target\"] , axis=1) # Complete the code to remove the column named 'Target'\n",
        "y = data[\"Target\"] # Complete the code to select the column named 'Target'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eBXiW4s4kd63"
      },
      "source": [
        "**Since we already have a separate test set, we don't need to divide data into train, valiation and test**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j-8vqBMXbp09"
      },
      "outputs": [],
      "source": [
        "# Splitting data into training and validation set:\n",
        "\n",
        "X_train, X_val, y_train, y_val = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=1, stratify=y # Complete the code to define the test size\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hIsrQQVqRbnY"
      },
      "outputs": [],
      "source": [
        "# Checking the number of rows and columns in the X_train data\n",
        "X_train.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o3_X_6ZicNfN"
      },
      "outputs": [],
      "source": [
        "# Checking the number of rows and columns in the X_val data\n",
        "X_val.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IktO9G2Lbp09"
      },
      "outputs": [],
      "source": [
        "# Dividing test data into X_test and y_test\n",
        "X_test = data_test.drop(columns = ['Target'] , axis= 1) # Complete the code to remove the target column\n",
        "y_test = data_test[\"Target\"] # Complete the code to select the target column"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XmuB5K_9b8tP"
      },
      "outputs": [],
      "source": [
        "# Checking the number of rows and columns in the X_test data\n",
        "X_test.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hRIcs_56ECHk"
      },
      "source": [
        "## Missing Value Imputation\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0J99-7Kubp09"
      },
      "source": [
        "- There were few missing values in V1 and V2, we will impute them using the median.\n",
        "- And to avoid data leakage we will impute missing values after splitting train data into train and validation sets.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "28L1vgAwbp09"
      },
      "outputs": [],
      "source": [
        "imputer = SimpleImputer(strategy=\"median\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FNfbw4rJbp09"
      },
      "outputs": [],
      "source": [
        "# Fit and transform the train data\n",
        "X_train = pd.DataFrame(imputer.fit_transform(X_train), columns=X_train.columns)\n",
        "\n",
        "# Transform the validation data\n",
        "X_val = pd.DataFrame(imputer.transform(X_val), columns=X_train.columns)    # Complete the code to impute missing values in the validation set while accounting for data leakage\n",
        "\n",
        "# Transform the test data\n",
        "X_test = pd.DataFrame(imputer.transform(X_test), columns=X_train.columns)    # Complete the code to impute missing values in the test set while accounting for data leakage"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3jdLvwTAbp09"
      },
      "outputs": [],
      "source": [
        "# Checking that no column has missing values in train or test sets\n",
        "print(X_train.isna().sum())\n",
        "print(\"-\" * 30)\n",
        "print(X_val.isna().sum())\n",
        "print(\"-\" * 30)\n",
        "print(X_test.isna().sum())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vdAf7_aNRpyP"
      },
      "outputs": [],
      "source": [
        "# y_train = y_train.to_numpy()\n",
        "# y_val = y_val.to_numpy()\n",
        "# y_test = y_test.to_numpy()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OzOa9FGA6WtG"
      },
      "source": [
        "# **Model Building**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "czwxDIZNccQL"
      },
      "source": [
        "## Model Evaluation Criterion"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UvyfsGJieL7U"
      },
      "source": [
        "This is a Precision-Recall vs. Threshold curve that shows how your model's performance changes as you adjust the decision threshold. Here's how to read it:\n",
        "The Axes:\n",
        "\n",
        "X-axis: Decision threshold (0.0 to 1.0)\n",
        "Y-axis: Performance scores (0.0 to 1.0)\n",
        "Blue line: Precision at each threshold\n",
        "Orange line: Recall at each threshold\n",
        "\n",
        "Key Patterns:\n",
        "As threshold increases (moving right):\n",
        "\n",
        "Precision generally increases (blue line goes up)\n",
        "Recall generally decreases (orange line goes down)\n",
        "\n",
        "What This Means:\n",
        "Low thresholds (left side, ~0.0-0.3):\n",
        "\n",
        "Model predicts \"positive\" for almost everything\n",
        "High recall (~1.0) - catches nearly all positive cases\n",
        "Low precision (~0.1) - lots of false positives\n",
        "\n",
        "High thresholds (right side, ~0.7-1.0):\n",
        "\n",
        "Model is very conservative, only predicts \"positive\" when very confident\n",
        "High precision (~1.0) - few false positives\n",
        "Low recall (~0.2) - misses many positive cases\n",
        "\n",
        "How to Use This:\n",
        "\n",
        "Find the crossover point (~0.75 threshold) where precision and recall are roughly equal\n",
        "Choose based on your needs:\n",
        "\n",
        "Need high recall? Choose lower threshold (~0.1-0.3)\n",
        "Need high precision? Choose higher threshold (~0.8-0.9)\n",
        "Balanced approach? Choose around the crossover point\n",
        "\n",
        "\n",
        "\n",
        "This curve helps you pick the optimal threshold based on whether false positives or false negatives are more costly in your specific use case."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l2ORUgmUjDZC"
      },
      "source": [
        "Metric of Choice: Recall\n",
        "Rationale:\n",
        "The primary objective of this predictive maintenance system is to identify wind turbine generator failures before they occur to minimize operational costs. The business context clearly establishes a cost hierarchy that makes Recall the most appropriate evaluation metric.\n",
        "According to the problem statement:\n",
        "\n",
        "\"It is given that the cost of repairing a generator is much less than the cost of replacing it, and the cost of inspection is less than the cost of repair.\"\n",
        "\n",
        "This creates the following cost structure:\n",
        "\n",
        "Inspection costs (False Positives) < Repair costs (True Positives) << Replacement costs (False Negatives)\n",
        "\n",
        "The problem further explains the business impact of different prediction outcomes:\n",
        "\n",
        "\"True positives (TP) are failures correctly predicted by the model. These will result in repairing costs.\n",
        "False negatives (FN) are real failures where there is no detection by the model. These will result in replacement costs.\n",
        "False positives (FP) are detections where there is no failure. These will result in inspection costs.\"\n",
        "\n",
        "Since replacement costs are significantly higher than repair costs (\"much less\"), minimizing False Negatives is critical. Missing a real failure (False Negative) leads to complete generator replacement, which is the most expensive outcome. In contrast, False Positives only result in unnecessary inspections, which are the least costly.\n",
        "Recall maximizes the detection of actual failures, ensuring that genuine failure cases are not missed, thereby avoiding the expensive replacement scenario. While this may increase False Positives (unnecessary inspections), the cost trade-off strongly favors this approach given the substantial difference between inspection and replacement costs.\n",
        "Therefore, Recall is the optimal metric for this predictive maintenance classification problem.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XWoHuUpjbp0_"
      },
      "source": [
        "**We are now done with pre-processing and evaluation criterion, so let's start building the model.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hoFZRy7tSXnp"
      },
      "source": [
        "## Utility Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fpMmFI9USZYW"
      },
      "outputs": [],
      "source": [
        "def plot(history, name):\n",
        "    \"\"\"\n",
        "    Function to plot loss/accuracy\n",
        "\n",
        "    history: an object which stores the metrics and losses.\n",
        "    name: can be one of Loss or Accuracy\n",
        "    \"\"\"\n",
        "    fig, ax = plt.subplots() #Creating a subplot with figure and axes.\n",
        "    plt.plot(history.history[name]) #Plotting the train accuracy or train loss\n",
        "    plt.plot(history.history['val_'+name]) #Plotting the validation accuracy or validation loss\n",
        "\n",
        "    plt.title('Model ' + name.capitalize()) #Defining the title of the plot.\n",
        "    plt.ylabel(name.capitalize()) #Capitalizing the first letter.\n",
        "    plt.xlabel('Epoch') #Defining the label for the x-axis.\n",
        "    fig.legend(['Train', 'Validation'], loc=\"outside right upper\") #Defining the legend, loc controls the position of the legend."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H_VAOcNcScFz"
      },
      "outputs": [],
      "source": [
        "# defining a function to compute different metrics to check performance of a classification model built using statsmodels\n",
        "def model_performance_classification(\n",
        "    model, predictors, target, threshold=0.5\n",
        "):\n",
        "    \"\"\"\n",
        "    Function to compute different metrics to check classification model performance\n",
        "\n",
        "    model: classifier\n",
        "    predictors: independent variables\n",
        "    target: dependent variable\n",
        "    threshold: threshold for classifying the observation as class 1\n",
        "    \"\"\"\n",
        "\n",
        "    # checking which probabilities are greater than threshold\n",
        "    pred = model.predict(predictors) > threshold\n",
        "    # pred_temp = model.predict(predictors) > threshold\n",
        "    # # rounding off the above values to get classes\n",
        "    # pred = np.round(pred_temp)\n",
        "\n",
        "    acc = accuracy_score(target, pred)  # to compute Accuracy\n",
        "    recall = recall_score(target, pred, average='macro')  # to compute Recall\n",
        "    precision = precision_score(target, pred, average='macro')  # to compute Precision\n",
        "    f1 = f1_score(target, pred, average='macro')  # to compute F1-score\n",
        "\n",
        "    # creating a dataframe of metrics\n",
        "    df_perf = pd.DataFrame(\n",
        "        {\"Accuracy\": acc, \"Recall\": recall, \"Precision\": precision, \"F1 Score\": f1,}, index = [0]\n",
        "    )\n",
        "\n",
        "    return df_perf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4fI98GOV0pTY"
      },
      "source": [
        "## Initial Model Building (Model 0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0usUom8nZOYZ"
      },
      "source": [
        "- Let's start with a neural network consisting of\n",
        "  - just one hidden layer of 7 neurons respectively\n",
        "  - activation function of ReLU.\n",
        "  - SGD as the optimizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-OTOG3o1bp0_"
      },
      "outputs": [],
      "source": [
        "# defining the batch size and # epochs upfront as we'll be using the same values for all models\n",
        "epochs = 10    # Complete the code to enter the number of epochs to be used in all models\n",
        "batch_size = 32    # Complete the code to enter the batch size to be used in all models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cm61fR_mbp0_"
      },
      "outputs": [],
      "source": [
        "# clears the current Keras session, resetting all layers and models previously created, freeing up memory and resources.\n",
        "tf.keras.backend.clear_session()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ao_7u1sOZlZz"
      },
      "outputs": [],
      "source": [
        "#Initializing the neural network\n",
        "model_0 = Sequential()\n",
        "model_0.add(Dense( 7 ,activation=\"relu\",input_dim=X_train.shape[1])) # Complete the code to define the number of neurons and the activation function\n",
        "model_0.add(Dense( 1 ,activation=\"sigmoid\")) # Complete the code to define the number of neurons in the output layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o2dEk01hbp1A"
      },
      "outputs": [],
      "source": [
        "model_0.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZsOer36qbp1A"
      },
      "outputs": [],
      "source": [
        "optimizer = tf.keras.optimizers.SGD()   # defining SGD as the optimizer to be used\n",
        "#model_0.compile(loss='binary_crossentropy', optimizer=optimizer, metrics = ['accuracy']) ## Uncomment this line in case the metric of choice is Accuracy\n",
        "#model_0.compile(loss='binary_crossentropy', optimizer=optimizer, metrics = ['Precision']) ## Uncomment this line in case the metric of choice is Precision\n",
        "model_0.compile(loss='binary_crossentropy', optimizer=optimizer, metrics = ['Recall']) ## Uncomment this line in case the metric of choice is Recall\n",
        "#model_0.compile(loss='binary_crossentropy', optimizer=optimizer, metrics = ['f1_score']) ## Uncomment this line in case the metric of choice is F1 Score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2BAFjvt_TB-a"
      },
      "outputs": [],
      "source": [
        "optimizer = tf.keras.optimizers.SGD()   # defining SGD as the optimizer to be used\n",
        "model_0.compile(loss='binary_crossentropy', optimizer=optimizer, metrics = ['Recall']) ## Uncomment this line in case the metric of choice is Recall"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kwUcBORjbp1A"
      },
      "outputs": [],
      "source": [
        "start = time.time()\n",
        "history = model_0.fit(X_train, y_train, validation_data=(X_val,y_val) , batch_size=batch_size, epochs=epochs)\n",
        "end=time.time()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zwBB3mRhcCN6"
      },
      "outputs": [],
      "source": [
        "print(\"Time taken in seconds \",end-start)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RfLx8ubqTe_n"
      },
      "outputs": [],
      "source": [
        "plot(history,'loss')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pr6dBVPNUe3Y"
      },
      "outputs": [],
      "source": [
        "model_0_train_perf = model_performance_classification(model_0, X_train, y_train)\n",
        "model_0_train_perf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1pdgAup2UfAX"
      },
      "outputs": [],
      "source": [
        "model_0_val_perf = model_performance_classification(model_0,X_val,y_val)\n",
        "model_0_val_perf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7757b8PflQ9T"
      },
      "source": [
        "Let's check the classification reports."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m0UQ_ZW-KCaX"
      },
      "outputs": [],
      "source": [
        "y_train_pred_0 = model_0.predict(X_train)\n",
        "y_val_pred_0 = model_0.predict(X_val)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zC6fR5ohKFzh"
      },
      "outputs": [],
      "source": [
        "print(\"Classification Report - Train data Model_0\",end=\"\\n\\n\")\n",
        "cr_train_model_0 = classification_report(y_train,y_train_pred_0>0.5)\n",
        "print(cr_train_model_0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "foiSZUcHKFp6"
      },
      "outputs": [],
      "source": [
        "print(\"Classification Report - Validation data Model_0\",end=\"\\n\\n\")\n",
        "cr_val_model_0 = classification_report(y_val,y_val_pred_0>0.5)\n",
        "print(cr_val_model_0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2tlVEbIWKb6T"
      },
      "source": [
        "# **Model Performance Improvement**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "grQ6pRK1WZtr"
      },
      "source": [
        "## Model 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zBr3Eo1iWhLs"
      },
      "source": [
        "- Let's try adding another layer to see if we can improve our model's performance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E95a3_v6UfE7"
      },
      "outputs": [],
      "source": [
        "# clears the current Keras session, resetting all layers and models previously created, freeing up memory and resources.\n",
        "tf.keras.backend.clear_session()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uzB8avdtUfH_"
      },
      "outputs": [],
      "source": [
        "#Initializing the neural network\n",
        "model_1 = Sequential()\n",
        "model_1.add(Dense( 64 ,activation=\"relu\",input_dim=X_train.shape[1])) # Complete the code to define the number of neurons and activation function\n",
        "model_1.add(Dense( 32 ,activation=\"relu\")) # Complete the code to define the number of neurons and activation function\n",
        "model_1.add(Dense(1,activation=\"sigmoid\")) # Complete the code to define the number of neurons in the output layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cnuMH8oQUfKf"
      },
      "outputs": [],
      "source": [
        "model_1.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xfDQJHYnUfM1"
      },
      "outputs": [],
      "source": [
        "optimizer = tf.keras.optimizers.SGD()   # defining SGD as the optimizer to be used\n",
        "# model_1.compile(loss='binary_crossentropy', optimizer=optimizer, metrics = ['accuracy']) ## Uncomment this line in case the metric of choice is Accuracy\n",
        "# model_1.compile(loss='binary_crossentropy', optimizer=optimizer, metrics = ['Precision']) ## Uncomment this line in case the metric of choice is Precision\n",
        "model_1.compile(loss='binary_crossentropy', optimizer=optimizer, metrics = ['Recall']) ## Uncomment this line in case the metric of choice is Recall\n",
        "# model_1.compile(loss='binary_crossentropy', optimizer=optimizer, metrics = ['f1_score']) ## Uncomment this line in case the metric of choice is F1 Score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HsPSnSmNUfO4"
      },
      "outputs": [],
      "source": [
        "start = time.time()\n",
        "history = model_1.fit(X_train, y_train, validation_data=(X_val,y_val) , batch_size=batch_size, epochs=epochs)\n",
        "end=time.time()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GX9bBylsUfRN"
      },
      "outputs": [],
      "source": [
        "print(\"Time taken in seconds \",end-start)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pKXH3xAYUfTP"
      },
      "outputs": [],
      "source": [
        "plot(history,'loss')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ve7QYfyTUfXn"
      },
      "outputs": [],
      "source": [
        "model_1_train_perf = model_performance_classification(model_1,X_train,y_train)\n",
        "model_1_train_perf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "09KWUhmONOEf"
      },
      "outputs": [],
      "source": [
        "model_1_val_perf = model_performance_classification(model_1,X_val,y_val)\n",
        "model_1_val_perf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DS8SFCwJUoe6"
      },
      "outputs": [],
      "source": [
        "y_train_pred_1 = model_1.predict(X_train)\n",
        "y_val_pred_1 = model_1.predict(X_val)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8ZngTFbEUskK"
      },
      "outputs": [],
      "source": [
        "print(\"Classification Report - Train data Model_1\", end=\"\\n\\n\")\n",
        "cr_train_model_1 = classification_report(y_train,y_train_pred_1 > 0.5)\n",
        "print(cr_train_model_1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ju0Rzn1kUtRi"
      },
      "outputs": [],
      "source": [
        "print(\"Classification Report - Validation data Model_1\", end=\"\\n\\n\")\n",
        "cr_val_model_1 = classification_report(y_val,y_val_pred_1 > 0.5)\n",
        "print(cr_val_model_1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iQhdyBg0OJWQ"
      },
      "source": [
        "## Model 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jUwTGnuGfgRp"
      },
      "source": [
        "To introduce Regularization in our model, let's set the dropout to 50% after adding the first hidden layer. This step will randomly drop 50% of the neurons before proceeding to the next layer, reducing overfitting."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-7mdqo7xOO_b"
      },
      "outputs": [],
      "source": [
        "# clears the current Keras session, resetting all layers and models previously created, freeing up memory and resources.\n",
        "tf.keras.backend.clear_session()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TXbGBf1QOO9B"
      },
      "outputs": [],
      "source": [
        "#Initializing the neural network\n",
        "from tensorflow.keras.layers import Dropout\n",
        "model_2 = Sequential()\n",
        "model_2.add(Dense(64,activation=\"relu\",input_dim=X_train.shape[1]))  # Complete the code to define the number of neurons and activation function\n",
        "model_2.add(Dropout(0.5)) # Complete the code to define the dropout rate\n",
        "model_2.add(Dense(32,activation = \"relu\")) # Complete the code to define the number of neurons and activation function\n",
        "model_2.add(Dense(16,activation = \"relu\")) # Complete the code to define the number of neurons and activation function\n",
        "model_2.add(Dense(1,activation=\"sigmoid\")) # Complete the code to define the number of neurons in the output layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nPu7lC9pOO23"
      },
      "outputs": [],
      "source": [
        "model_2.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U7-Y_BmYOOxB"
      },
      "outputs": [],
      "source": [
        "optimizer = tf.keras.optimizers.SGD()   # defining SGD as the optimizer to be used\n",
        "# model_2.compile(loss='binary_crossentropy', optimizer=optimizer, metrics = ['accuracy']) ## Uncomment this line in case the metric of choice is Accuracy\n",
        "# model_2.compile(loss='binary_crossentropy', optimizer=optimizer, metrics = ['Precision']) ## Uncomment this line in case the metric of choice is Precision\n",
        "model_2.compile(loss='binary_crossentropy', optimizer=optimizer, metrics = ['Recall']) ## Uncomment this line in case the metric of choice is Recall\n",
        "# model_2.compile(loss='binary_crossentropy', optimizer=optimizer, metrics = ['f1_score']) ## Uncomment this line in case the metric of choice is F1 Score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_QPtdXotOOu0"
      },
      "outputs": [],
      "source": [
        "start = time.time()\n",
        "history = model_2.fit(X_train, y_train, validation_data=(X_val,y_val) , batch_size=batch_size, epochs=epochs)\n",
        "end=time.time()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a_VLN5_FOOss"
      },
      "outputs": [],
      "source": [
        "print(\"Time taken in seconds \",end-start)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1yat6HgCOOmV"
      },
      "outputs": [],
      "source": [
        "plot(history,'loss')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fdmnaDUxbVvF"
      },
      "source": [
        "Lets check the model performance of model_2 on training and validation data respectively."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lhHFHpRJOOf1"
      },
      "outputs": [],
      "source": [
        "model_2_train_perf = model_performance_classification(model_2,X_train,y_train)\n",
        "model_2_train_perf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pnMTEzWtOOW0"
      },
      "outputs": [],
      "source": [
        "model_2_val_perf = model_performance_classification(model_2,X_val,y_val)\n",
        "model_2_val_perf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DI5dqTACWC5N"
      },
      "outputs": [],
      "source": [
        "y_train_pred_2 = model_2.predict(X_train)\n",
        "y_val_pred_2 = model_2.predict(X_val)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XZOkixQ9bfnI"
      },
      "source": [
        "Lets check the classification report of model_2 on training and validation data respectively."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TPtFhE5GWAFM"
      },
      "outputs": [],
      "source": [
        "print(\"Classification Report - Train data Model_2\", end=\"\\n\\n\")\n",
        "cr_train_model_2 = classification_report(y_train,y_train_pred_2 > 0.5)\n",
        "print(cr_train_model_2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cE-wWumtWEpx"
      },
      "outputs": [],
      "source": [
        "print(\"Classification Report - Validation data Model_2\", end=\"\\n\\n\")\n",
        "cr_val_model_2 = classification_report(y_val , y_val_pred_2 > 0.5)\n",
        "print(cr_val_model_2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "swQ14MhSRwCc"
      },
      "source": [
        "As we have are dealing with an imbalance in class distribution, we should also be using class weights to allow the model to give proportionally more importance to the minority class."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bXFEE44vOkYz"
      },
      "outputs": [],
      "source": [
        "# Calculate class weights for imbalanced dataset\n",
        "cw = (y_train.shape[0]) / np.bincount(y_train.astype(int)) # Convert y_train to integers\n",
        "\n",
        "# Create a dictionary mapping class indices to their respective class weights\n",
        "cw_dict = {}\n",
        "for i in range(cw.shape[0]):\n",
        "    cw_dict[i] = cw[i]\n",
        "\n",
        "cw_dict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hof3fAu9OkWD"
      },
      "outputs": [],
      "source": [
        "# clears the current Keras session, resetting all layers and models previously created, freeing up memory and resources.\n",
        "tf.keras.backend.clear_session()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PWebuv4mOkTC"
      },
      "outputs": [],
      "source": [
        "model_3 = Sequential()\n",
        "model_3.add(Dense(64,activation=\"relu\",input_dim=X_train.shape[1])) # Complete the code to define the number of neurons and activation function\n",
        "model_3.add(Dropout(0.5)) # Complete the code to define the dropout rate\n",
        "model_3.add(Dense(32,activation=\"relu\")) # Complete the code to define the number of neurons and activation function\n",
        "model_3.add(Dense(16, activation = \"relu\")) # Complete the code to define the number of neurons and activation function\n",
        "model_3.add(Dense(1,activation=\"sigmoid\")) # Complete the code to define the number of neurons in the output layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CN5XaqlQOkL1"
      },
      "outputs": [],
      "source": [
        "model_3.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vbe-Uvf_OkBd"
      },
      "outputs": [],
      "source": [
        "optimizer = tf.keras.optimizers.SGD()   # defining SGD as the optimizer to be used\n",
        "# model_3.compile(loss='binary_crossentropy', optimizer=optimizer, metrics = ['accuracy']) ## Uncomment this line in case the metric of choice is Accuracy\n",
        "# model_3.compile(loss='binary_crossentropy', optimizer=optimizer, metrics = ['Precision']) ## Uncomment this line in case the metric of choice is Precision\n",
        "model_3.compile(loss='binary_crossentropy', optimizer=optimizer, metrics = ['Recall']) ## Uncomment this line in case the metric of choice is Recall\n",
        "# model_3.compile(loss='binary_crossentropy', optimizer=optimizer, metrics = ['f1_score']) ## Uncomment this line in case the metric of choice is F1 Score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aGEYYC5pW5iM"
      },
      "outputs": [],
      "source": [
        "# Compile the model\n",
        "optimizer = tf.keras.optimizers.SGD()\n",
        "model_3.compile(loss='binary_crossentropy', optimizer=optimizer, metrics = ['Recall'])\n",
        "\n",
        "# Train the model\n",
        "start = time.time()\n",
        "history = model_3.fit(X_train, y_train, validation_data=(X_val,y_val) , batch_size=batch_size, epochs=epochs,class_weight=cw_dict)\n",
        "end=time.time()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-H-rbOvIW5gJ"
      },
      "outputs": [],
      "source": [
        "print(\"Time taken in seconds \",end-start)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aWkRSkTGW5dD"
      },
      "outputs": [],
      "source": [
        "plot(history,'loss')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dY6ZCgMGcUOb"
      },
      "source": [
        "Lets check the model performance of model_3 on training and validation data respectively."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_fryTkbgW5XE"
      },
      "outputs": [],
      "source": [
        "model_3_train_perf = model_performance_classification(model_3,X_train,y_train)\n",
        "model_3_train_perf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GxmCeXa1W5Oo"
      },
      "outputs": [],
      "source": [
        "model_3_val_perf = model_performance_classification(model_3,X_val,y_val)\n",
        "model_3_val_perf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1BkGKOeCXUsT"
      },
      "outputs": [],
      "source": [
        "y_train_pred_3 = model_3.predict(X_train)\n",
        "y_val_pred_3 = model_3.predict(X_val)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PMljwQUdcbNm"
      },
      "source": [
        "Lets check the classification report of model_3 on training and validation data respectively."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yUArAjvkXeh9"
      },
      "outputs": [],
      "source": [
        "print(\"Classification Report - Train data Model_3\", end=\"\\n\\n\")\n",
        "cr_train_model_3 = classification_report(y_train,y_train_pred_3 > 0.5)\n",
        "print(cr_train_model_3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mp5gmnufXhmy"
      },
      "outputs": [],
      "source": [
        "print(\"Classification Report - Validation data Model_3\", end=\"\\n\\n\")\n",
        "cr_val_model_3 = classification_report(y_val,y_val_pred_3 > 0.5)\n",
        "print(cr_val_model_3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QLLlTEPvnohY"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dPAuEVDzX0qL"
      },
      "source": [
        "## Model 4"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ydPkh3AdgWRU"
      },
      "source": [
        "Since we have used only SGD optimizer till now, let's use another kind of optimizer and observe its impact on the model performmance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q9Jx5vOFX0qM"
      },
      "outputs": [],
      "source": [
        "# clears the current Keras session, resetting all layers and models previously created, freeing up memory and resources.\n",
        "tf.keras.backend.clear_session()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q_LP0NHzX0qP"
      },
      "outputs": [],
      "source": [
        "#Initializing the neural network\n",
        "model_4 = Sequential()\n",
        "model_4.add(Dense(64,activation=\"relu\",input_dim=X_train.shape[1])) # Complete the code to define the number of neurons and activation function\n",
        "model_4.add(Dense(32,activation=\"relu\")) # Complete the code to define the number of neurons and activation function\n",
        "model_4.add(Dense(1,activation=\"sigmoid\")) # Complete the code to define the number of neurons in the output layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MY_drKH7X0qP"
      },
      "outputs": [],
      "source": [
        "model_4.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vbXDHdCaX0qQ"
      },
      "outputs": [],
      "source": [
        "optimizer = tf.keras.optimizers.Adam()    # defining Adam as the optimizer to be used\n",
        "# model_4.compile(loss='binary_crossentropy', optimizer=optimizer, metrics = ['accuracy']) ## Uncomment this line in case the metric of choice is Accuracy\n",
        "# model_4.compile(loss='binary_crossentropy', optimizer=optimizer, metrics = ['Precision']) ## Uncomment this line in case the metric of choice is Precision\n",
        "model_4.compile(loss='binary_crossentropy', optimizer=optimizer, metrics = ['Recall']) ## Uncomment this line in case the metric of choice is Recall\n",
        "# model_4.compile(loss='binary_crossentropy', optimizer=optimizer, metrics = ['f1_score']) ## Uncomment this line in case the metric of choice is F1 Score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Viy8MTihX0qQ"
      },
      "outputs": [],
      "source": [
        "start = time.time()\n",
        "history = model_4.fit(X_train, y_train, validation_data=(X_val,y_val) , batch_size=batch_size, epochs=epochs)\n",
        "end=time.time()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uC_tPwLlX0qR"
      },
      "outputs": [],
      "source": [
        "print(\"Time taken in seconds \",end-start)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xJ1TAwlLX0qR"
      },
      "outputs": [],
      "source": [
        "plot(history,'loss')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E_KuUcLbdKRb"
      },
      "source": [
        "Lets check the model performance ofr model_4 on training and validation data respectively"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8OvkmGu6X0qR"
      },
      "outputs": [],
      "source": [
        "model_4_train_perf = model_performance_classification(model_4,X_train,y_train)\n",
        "model_4_train_perf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sLE1b8CMX0qR"
      },
      "outputs": [],
      "source": [
        "model_4_val_perf = model_performance_classification(model_4,X_val,y_val)\n",
        "model_4_val_perf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wg1dMCXSX0qS"
      },
      "outputs": [],
      "source": [
        "y_train_pred_4 = model_4.predict(X_train)\n",
        "y_val_pred_4 = model_4.predict(X_val)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4OH8cvF2dVjv"
      },
      "source": [
        "Lets check the classification report of model_4 on raining and validation data respectively."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D-wxc08wX0qS"
      },
      "outputs": [],
      "source": [
        "print(\"Classification Report - Train data Model_4\", end=\"\\n\\n\")\n",
        "cr_train_model_4 = classification_report(y_train,y_train_pred_4 > 0.5)\n",
        "print(cr_train_model_4)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zFAAplEuo5Uz"
      },
      "source": [
        "Comparison and Observations:\n",
        "\n",
        "Recall: The Recall for the minority class (1, failure) is slightly lower on the validation set (0.88) compared to the training set (0.91). This suggests a small drop in the model's ability to identify actual failures on unseen data.\n",
        "Precision: The Precision for the minority class is also slightly lower on the validation set (0.97) compared to the training set (0.98).\n",
        "Overall Performance: The overall Accuracy is the same (0.99) for both sets. The macro and weighted averages for Precision and F1-score are very close between training and validation.\n",
        "Conclusion:\n",
        "\n",
        "Model 4, using the Adam optimizer, shows high performance on both training and validation sets, achieving a high Recall on the training set (0.91). Similar to Model 1 and Model 3, there is a slight drop in Recall on the validation set, indicating a small degree of overfitting. However, the overall performance remains strong. Comparing it to Model 3 (which used SGD with class weights), Model 4 achieved a slightly higher Recall on the training set (0.91 vs 0.90) and a similar Recall on the validation set (0.88 vs 0.87). The choice between these models might depend on the tolerance for a slight drop in Recall on unseen data versus the potential benefits of using class weights or a different optimizer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uyKOGZzEX0qT"
      },
      "outputs": [],
      "source": [
        "print(\"Classification Report - Validation data Model_4\", end=\"\\n\\n\")\n",
        "cr_val_model_4 = classification_report(y_val,y_val_pred_4 > 0.5)\n",
        "print(cr_val_model_4)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W2dZjgH0Yxok"
      },
      "source": [
        "## Model 5"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x9H_G0rzdwC9"
      },
      "source": [
        "This time we will add more layers and dropout while using a different optimizer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WBKvvEkjYxol"
      },
      "outputs": [],
      "source": [
        "# clears the current Keras session, resetting all layers and models previously created, freeing up memory and resources.\n",
        "tf.keras.backend.clear_session()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qrPwAVJOYxom"
      },
      "outputs": [],
      "source": [
        "#Initializing the neural network\n",
        "from tensorflow.keras.layers import Dropout\n",
        "model_5 = Sequential()\n",
        "model_5.add(Dense(64,activation=\"relu\",input_dim=X_train.shape[1])) # Complete the code to define the number of neurons and activation function\n",
        "model_5.add(Dropout(0.5)) #Complete the code to define the dropout rate\n",
        "model_5.add(Dense(32,activation=\"relu\")) # Complete the code to define the number of neurons and activation function\n",
        "model_5.add(Dense(16, activation = \"relu\")) # Complete the code to define the number of neurons and activation function\n",
        "model_5.add(Dense(1,activation=\"sigmoid\")) # Complete the code to define the number of neurons in the output layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jU1Q8Z8CYxom"
      },
      "outputs": [],
      "source": [
        "model_5.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6MQXRa63Yxom"
      },
      "outputs": [],
      "source": [
        "optimizer = tf.keras.optimizers.Adam()    # defining Adam as the optimizer to be used\n",
        "# model_5.compile(loss='binary_crossentropy', optimizer=optimizer, metrics = ['accuracy']) ## Uncomment this line in case the metric of choice is Accuracy\n",
        "# model_5.compile(loss='binary_crossentropy', optimizer=optimizer, metrics = ['Precision']) ## Uncomment this line in case the metric of choice is Precision\n",
        "model_5.compile(loss='binary_crossentropy', optimizer=optimizer, metrics = ['Recall']) ## Uncomment this line in case the metric of choice is Recall\n",
        "# model_5.compile(loss='binary_crossentropy', optimizer=optimizer, metrics = ['f1_score']) ## Uncomment this line in case the metric of choice is F1 Score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4nfW1HAYYxon"
      },
      "outputs": [],
      "source": [
        "start = time.time()\n",
        "history = model_5.fit(X_train, y_train, validation_data=(X_val,y_val) , batch_size=batch_size, epochs=epochs)\n",
        "end=time.time()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iT96bkirYxon"
      },
      "outputs": [],
      "source": [
        "print(\"Time taken in seconds \",end-start)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lV6Ve4KuYxon"
      },
      "outputs": [],
      "source": [
        "plot(history,'loss')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DL-94xameUkb"
      },
      "source": [
        "Lets check the model performance of model_5 on the training and validation data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LRiDnwPhYxoo"
      },
      "outputs": [],
      "source": [
        "model_5_train_perf = model_performance_classification(model_5,X_train,y_train)\n",
        "model_5_train_perf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hyz9ix7KYxoo"
      },
      "outputs": [],
      "source": [
        "model_5_val_perf = model_performance_classification(model_5,X_val,y_val)\n",
        "model_5_val_perf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B0aSAUTlYxop"
      },
      "outputs": [],
      "source": [
        "y_train_pred_5 = model_5.predict(X_train)\n",
        "y_val_pred_5 = model_5.predict(X_val)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "68DKFcJIehp1"
      },
      "source": [
        "Lets check the classification report of model_5 on training and validation data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i9XpJpekYxop"
      },
      "outputs": [],
      "source": [
        "print(\"Classification Report - Train data Model_2\", end=\"\\n\\n\")\n",
        "cr_train_model_5 = classification_report(y_train,y_train_pred_5 > 0.5)\n",
        "print(cr_train_model_5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9RlRQJOuYxop"
      },
      "outputs": [],
      "source": [
        "print(\"Classification Report - Validation data Model_2\", end=\"\\n\\n\")\n",
        "cr_val_model_5 = classification_report(y_val,y_val_pred_5 > 0.5)\n",
        "print(cr_val_model_5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H-wDd0RIZQyI"
      },
      "source": [
        "## Model 6"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p0IOkrVWYxop"
      },
      "source": [
        "Let's see how does the model performance change when the model gives higher importance to the minority class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0LOpw8cNZQyJ"
      },
      "outputs": [],
      "source": [
        "# clears the current Keras session, resetting all layers and models previously created, freeing up memory and resources.\n",
        "tf.keras.backend.clear_session()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HNUew9nFZQyJ"
      },
      "outputs": [],
      "source": [
        "model_6 = Sequential()\n",
        "model_6.add(Dense(64,activation=\"relu\",input_dim=X_train.shape[1])) # Complete the code to define the number of neurons and activation function\n",
        "model_6.add(Dropout(0.5)) # Complete the code to define the dropout rate\n",
        "model_6.add(Dense(32,activation=\"relu\")) # Complete the code to define the number of neurons and activation function\n",
        "model_6.add(Dense(16, activation = \"relu\")) # Complete the code to define the number of neurons and activation function\n",
        "model_6.add(Dense(1,activation=\"sigmoid\")) # Complete the code to define the number of neurons in the output layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h_sGGGREZQyJ"
      },
      "outputs": [],
      "source": [
        "#Initializing the neural network\n",
        "from tensorflow.keras.layers import Dropout\n",
        "model_6 = Sequential()\n",
        "model_6.add(Dense(64,activation=\"relu\",input_dim=X_train.shape[1])) # Complete the code to define the number of neurons and activation function\n",
        "model_6.add(Dropout(0.5)) # Complete the code to define the dropout rate\n",
        "model_6.add(Dense(32,activation=\"relu\")) # Complete the code to define the number of neurons and activation function\n",
        "model_6.add(Dense(16, activation = \"relu\")) # Complete the code to define the number of neurons and activation function\n",
        "model_6.add(Dense(1,activation=\"sigmoid\")) # Complete the code to define the number of neurons in the output layer\n",
        "\n",
        "model_6.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vaVP4CM9ZQyJ"
      },
      "outputs": [],
      "source": [
        "optimizer = tf.keras.optimizers.SGD()\n",
        "# model_6.compile(loss='binary_crossentropy', optimizer=optimizer, metrics = ['accuracy']) ## Uncomment this line in case the metric of choice is Accuracy\n",
        "# model_6.compile(loss='binary_crossentropy', optimizer=optimizer, metrics = ['Precision']) ## Uncomment this line in case the metric of choice is Precision\n",
        "model_6.compile(loss='binary_crossentropy', optimizer=optimizer, metrics = ['Recall']) ## Uncomment this line in case the metric of choice is Recall\n",
        "# model_6.compile(loss='binary_crossentropy', optimizer=optimizer, metrics = ['f1_score']) ## Uncomment this line in case the metric of choice is F1 Score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fhiZR8WMZQyJ"
      },
      "outputs": [],
      "source": [
        "start = time.time()\n",
        "history = model_3.fit(X_train, y_train, validation_data=(X_val,y_val) , batch_size=batch_size, epochs=epochs,class_weight=cw_dict, ) # Complete the code such that the model is biased towards the minority class\n",
        "end=time.time()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "13AF9v2UZQyJ"
      },
      "outputs": [],
      "source": [
        "print(\"Time taken in seconds \",end-start)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "48uod9TsZQyK"
      },
      "outputs": [],
      "source": [
        "plot(history,'loss')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YhlzTJsngBev"
      },
      "source": [
        "Lets check the model performance of model_6 on training and validation data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fiy0gmSUZQyK"
      },
      "outputs": [],
      "source": [
        "model_6_train_perf = model_performance_classification(model_6,X_train,y_train)\n",
        "model_6_train_perf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aX8Zu4FLZQyK"
      },
      "outputs": [],
      "source": [
        "model_6_val_perf = model_performance_classification(model_6,X_val,y_val)\n",
        "model_6_val_perf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IXZlBIDLZQyK"
      },
      "outputs": [],
      "source": [
        "y_train_pred_6 = model_6.predict(X_train)\n",
        "y_val_pred_6 = model_6.predict(X_val)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jNsmMIsxgJE_"
      },
      "source": [
        "Lets check the classification report of model_6 on both training and validation data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DXeX1QHSZQyK"
      },
      "outputs": [],
      "source": [
        "print(\"Classification Report - Train data Model_3\", end=\"\\n\\n\")\n",
        "cr_train_model_6 = classification_report(y_train,y_train_pred_6 > 0.5)\n",
        "print(cr_train_model_6)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SV3-H6ckZQyK"
      },
      "outputs": [],
      "source": [
        "print(\"Classification Report - Validation data Model_3\", end=\"\\n\\n\")\n",
        "cr_val_model_6 = classification_report(y_val,y_val_pred_6 > 0.5)\n",
        "print(cr_val_model_6)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3O07XgkzY_ot"
      },
      "source": [
        "# **Model Performance Comparison and Final Model Selection**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mVYagrg9ZQyK"
      },
      "source": [
        "Now, in order to select the final model, we will compare the performances of all the models for the training and test sets."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QfZUZD83lu4v"
      },
      "source": [
        "**Training Performance Comparison**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HQB0PVtbh-MA"
      },
      "outputs": [],
      "source": [
        "# training performance comparison\n",
        "\n",
        "models_train_comp_df = pd.concat(\n",
        "    [\n",
        "        model_0_train_perf.T,\n",
        "        model_1_train_perf.T,\n",
        "        model_2_train_perf.T,\n",
        "        model_3_train_perf.T,\n",
        "        model_4_train_perf.T,\n",
        "        model_5_train_perf.T,\n",
        "        model_6_train_perf.T\n",
        "\n",
        "    ],\n",
        "    axis=1,\n",
        ")\n",
        "models_train_comp_df.columns = [\n",
        "    \"Model 0\",\n",
        "    \"Model 1\",\n",
        "    \"Model 2\",\n",
        "    \"Model 3\",\n",
        "    \"Model 4\",\n",
        "    \"Model 5\",\n",
        "    \"Model 6\"\n",
        "]\n",
        "print(\"Training set performance comparison:\")\n",
        "models_train_comp_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jqpgdol7l2Um"
      },
      "source": [
        "**Validation Performance Comparison**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KzWl6tM8ilz9"
      },
      "outputs": [],
      "source": [
        "# Validation performance comparison\n",
        "\n",
        "models_val_comp_df = pd.concat(\n",
        "    [\n",
        "        model_0_val_perf.T,\n",
        "        model_1_val_perf.T,\n",
        "        model_2_val_perf.T,\n",
        "        model_3_val_perf.T,\n",
        "        model_4_val_perf.T,\n",
        "        model_5_val_perf.T,\n",
        "        model_6_val_perf.T\n",
        "\n",
        "    ],\n",
        "    axis=1,\n",
        ")\n",
        "models_val_comp_df.columns = [\n",
        "    \"Model 0\",\n",
        "    \"Model 1\",\n",
        "    \"Model 2\",\n",
        "    \"Model 3\",\n",
        "    \"Model 4\",\n",
        "    \"Model 5\",\n",
        "    \"Model 6\"\n",
        "]\n",
        "print(\"Validation set performance comparison:\")\n",
        "models_val_comp_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q69xcftQuKyg"
      },
      "source": [
        "**Checking the performance of the best model on the test set**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_pfViqPWq_pO"
      },
      "outputs": [],
      "source": [
        "# best_model = model_0 ## Uncomment this line in case the best model is model_0\n",
        "# best_model = model_1 ## Uncomment this line in case the best model is model_1\n",
        "# best_model = model_2 ## Uncomment this line in case the best model is model_2\n",
        "# best_model = model_3 ## Uncomment this line in case the best model is model_3\n",
        "best_model = model_4 ## Uncomment this line in case the best model is model_4\n",
        "# best_model = model_5 ## Uncomment this line in case the best model is model_5\n",
        "# best_model = model_6 ## Uncomment this line in case the best model is model_6"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oPxuwyh6j227"
      },
      "outputs": [],
      "source": [
        "# Test set performance for the best model\n",
        "best_model_test_perf = model_performance_classification(best_model,X_test,y_test)\n",
        "best_model_test_perf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AXaN2VVHxDej"
      },
      "source": [
        "Key reasons Model 4 is optimal:\n",
        "\n",
        "Highest validation recall (0.938261) - This is what matters most for real-world performance\n",
        "Minimal overfitting - Small gap between training (0.952835) and validation (0.938261) recall, indicating good generalization\n",
        "Well-balanced performance - Strong across all metrics, not just recall\n",
        "\n",
        "Why not the others:\n",
        "\n",
        "Model 3: Slightly lower validation recall (0.932429)\n",
        "Model 5: Much lower validation recall (0.916269) despite high training recall\n",
        "Models 0, 1, 2: Lower validation recall scores\n",
        "Model 6: Dramatically poor performance across all metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J4ZzDS8Cqe_b"
      },
      "outputs": [],
      "source": [
        "y_test_pred_best = best_model.predict(X_test)\n",
        "\n",
        "cr_test_best_model = classification_report(y_test, y_test_pred_best>0.5) # Check the classification report of best model on test data.\n",
        "print(cr_test_best_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IkJU-1M5uT5b"
      },
      "source": [
        "- Write down actionable insights here"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-wtCOf9-8sC1"
      },
      "outputs": [],
      "source": [
        "import shap\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Scale data (if not already done)\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Convert to DataFrame to preserve feature names\n",
        "X_train_scaled_df = pd.DataFrame(X_train_scaled, columns=X.columns)\n",
        "X_test_scaled_df = pd.DataFrame(X_test_scaled, columns=X.columns)\n",
        "\n",
        "# Use a small background sample (required by KernelExplainer)\n",
        "background = X_train_scaled_df.sample(100, random_state=42)\n",
        "X_explain = X_test_scaled_df.sample(50, random_state=42)\n",
        "\n",
        "# Create SHAP KernelExplainer\n",
        "explainer = shap.KernelExplainer(best_model.predict, background)\n",
        "\n",
        "# Compute SHAP values (slow, but accurate)\n",
        "shap_values = explainer.shap_values(X_explain)\n",
        "\n",
        "# Plot summary and bar charts\n",
        "shap.summary_plot(shap_values, X_explain, feature_names=X.columns.tolist())\n",
        "shap.summary_plot(shap_values, X_explain, feature_names=X.columns.tolist(), plot_type=\"bar\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "77a73c9b"
      },
      "source": [
        "**How is SHAP making the results better?**\n",
        "\n",
        "SHAP (SHapley Additive exPlanations) enhances your model results by making them **interpretable** and **actionable**. While your classification model (like the best Model 3 or Model 4) tells you *whether* a generator is likely to fail, SHAP tells you *why*.\n",
        "\n",
        "Here's how that makes the results better in the context of this predictive maintenance problem:\n",
        "\n",
        "1.  **Understanding Feature Importance:** SHAP values quantify the contribution of each feature to a prediction. Instead of just knowing *that* a failure is predicted, you can see *which* variables (V1, V2, etc.) had the biggest impact on that specific prediction. This helps you understand which sensor readings or derived features are most indicative of a potential failure.\n",
        "\n",
        "2.  **Explaining Individual Predictions:** This is a major benefit. For any single turbine, if the model predicts a failure, SHAP can show you exactly which feature values are pushing the prediction towards 'failure' and which are pushing it towards 'no failure'. This is invaluable for maintenance teams ‚Äì they can see *why* a specific turbine is flagged, allowing them to investigate the relevant components or conditions.\n",
        "\n",
        "3.  **Identifying Global Patterns:** SHAP summary plots can show you the overall impact and direction of influence for each feature across the entire dataset or a subset. You can see, for example, if high values of V1 generally increase the probability of failure, while low values decrease it. This provides broader insights into the underlying factors contributing to generator failures.\n",
        "\n",
        "4.  **Building Trust:** A \"black box\" model that just gives a prediction can be hard for domain experts to trust. By providing explanations through SHAP, you build confidence in the model's predictions, as stakeholders can see the reasoning behind them and validate it with their domain knowledge.\n",
        "\n",
        "5.  **Actionable Insights:** Knowing *why* a failure is predicted directly leads to actionable steps. If a specific set of sensor readings (features) consistently indicates failure according to SHAP, maintenance can focus on checking the components related to those sensors. This moves from simply predicting failure to enabling targeted, cost-effective preventative maintenance.\n",
        "\n",
        "In summary, SHAP transforms model predictions from just outputs into understandable diagnoses, helping you move beyond simply predicting failures to understanding their root causes and taking precise, cost-effective actions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5cb7b4db"
      },
      "outputs": [],
      "source": [
        "# ================================================\n",
        "# Confusion Matrix at Custom Threshold\n",
        "# ================================================\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "\n",
        "# Set custom threshold (example: 0.3 for higher recall)\n",
        "custom_threshold = 0.3\n",
        "y_pred_custom = (y_test_pred_best > custom_threshold).astype(int)\n",
        "\n",
        "# Generate confusion matrix\n",
        "cm = confusion_matrix(y_test, y_pred_custom)\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
        "disp.plot(cmap='Blues')\n",
        "plt.title(f'Confusion Matrix @ Threshold = {custom_threshold}')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "47dc083a"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import precision_recall_curve\n",
        "\n",
        "# Calculate precision, recall, and thresholds\n",
        "precision, recall, thresholds = precision_recall_curve(y_test, y_test_pred_best)\n",
        "\n",
        "# Note: The last threshold in the 'thresholds' array is often 0.0 and corresponds to the highest recall and lowest precision.\n",
        "# The 'precision' and 'recall' arrays have one more element than 'thresholds'.\n",
        "# The last element of 'precision' is 1.0 and the last element of 'recall' is 0.0 (for a threshold of 1.0, where no positive predictions are made).\n",
        "# The 'thresholds' array contains thresholds such that for threshold t, a sample is predicted as positive if its score is > t."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kVXekwN6esni"
      },
      "outputs": [],
      "source": [
        "# Set custom threshold to 0.1 to prioritize Recall\n",
        "custom_threshold_recall = 0.1\n",
        "y_pred_recall = (y_test_pred_best > custom_threshold_recall).astype(int)\n",
        "\n",
        "# Display Classification Report at this threshold\n",
        "from sklearn.metrics import classification_report\n",
        "print(f\"Classification Report @ Threshold = {custom_threshold_recall}\", end=\"\\n\\n\")\n",
        "print(classification_report(y_test, y_pred_recall))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aace21bf"
      },
      "outputs": [],
      "source": [
        "# Analyze Precision and Recall at different thresholds from the previously computed curve\n",
        "# The variables precision, recall, and thresholds were computed in cell 18915fb2\n",
        "\n",
        "# Find the threshold that gives the highest recall for a minimum precision (e.g., 0.5 or higher)\n",
        "# Or, find a threshold where the gain in recall is significant for a small drop in precision\n",
        "\n",
        "# Let's find the threshold where Recall is maximized while maintaining Precision >= 0.5)\n",
        "min_precision = 0.5\n",
        "optimal_threshold = 0.5 # Start with default\n",
        "max_recall = 0\n",
        "\n",
        "for i in range(len(thresholds)):\n",
        "    if precision[i] >= min_precision:\n",
        "        if recall[i] > max_recall:\n",
        "            max_recall = recall[i]\n",
        "            optimal_threshold = thresholds[i]\n",
        "\n",
        "print(f\"Threshold to maximize Recall while maintaining Precision >= {min_precision}: {optimal_threshold:.4f}\")\n",
        "print(f\"Corresponding Recall: {max_recall:.4f}\")\n",
        "# Find corresponding precision at the optimal_threshold\n",
        "# Use np.where to find the index of the optimal_threshold in the thresholds array\n",
        "# Need to handle the case where optimal_threshold might not be exactly in thresholds array (due to floating point)\n",
        "# Find the index of the threshold closest to optimal_threshold if exact match not found\n",
        "closest_threshold_index = np.argmin(np.abs(thresholds - optimal_threshold))\n",
        "print(f\"Corresponding Precision: {precision[closest_threshold_index]:.4f}\")\n",
        "\n",
        "\n",
        "# Alternatively, you could look for the point where the curve starts to drop steeply in Recall\n",
        "# This often indicates diminishing returns for lowering the threshold further.\n",
        "\n",
        "# Let's find the threshold where Recall is close to its maximum but Precision hasn't dropped drastically.\n",
        "# This often indicates diminishing returns for lowering the threshold further.\n",
        "# We can look for a threshold where the difference between Recall and Precision is minimized,\n",
        "# or where Recall is high and Precision is still reasonable.\n",
        "\n",
        "# Another approach: Find the threshold that maximizes the F1-score (harmonic mean of Precision and Recall)\n",
        "# This balances both metrics, which might be useful if both false positives and false negatives have costs.\n",
        "f1_scores = 2 * (precision * recall) / (precision + recall)\n",
        "# Handle NaN values where precision and recall are both 0\n",
        "f1_scores = np.nan_to_num(f1_scores)\n",
        "# Find the index of the maximum F1-score\n",
        "optimal_threshold_f1_index = np.argmax(f1_scores)\n",
        "optimal_threshold_f1 = thresholds[optimal_threshold_f1_index]\n",
        "max_f1 = f1_scores[optimal_threshold_f1_index] # Use the F1 score at the determined index\n",
        "\n",
        "\n",
        "print(f\"\\nThreshold that maximizes F1-score: {optimal_threshold_f1:.4f}\")\n",
        "print(f\"Corresponding F1-score: {max_f1:.4f}\")\n",
        "# Find corresponding precision and recall at this F1-max threshold\n",
        "print(f\"Corresponding Recall at F1-max threshold: {recall[optimal_threshold_f1_index]:.4f}\")\n",
        "print(f\"Corresponding Precision at F1-max threshold: {precision[optimal_threshold_f1_index]:.4f}\")\n",
        "\n",
        "\n",
        "# Based on the business objective (minimizing replacement costs >> inspection costs),\n",
        "# prioritizing Recall is key. The threshold around the point where Recall is high\n",
        "# but Precision hasn't plummeted is a good adaptive threshold.\n",
        "# Looking at the curve, a threshold around 0.3-0.4 might offer a good balance.\n",
        "# Let's calculate metrics at threshold 0.3 as an example:\n",
        "example_threshold = 0.3\n",
        "y_pred_example = (y_test_pred_best > example_threshold).astype(int)\n",
        "report_example = classification_report(y_test, y_pred_example, output_dict=True)\n",
        "\n",
        "print(f\"\\nClassification Report Metrics @ Threshold = {example_threshold}\")\n",
        "# Safely access metrics using .get()\n",
        "print(f\"Recall (Class 1): {report_example.get('1', {}).get('recall', 0.0):.4f}\")\n",
        "print(f\"Precision (Class 1): {report_example.get('1', {}).get('precision', 0.0):.4f}\")\n",
        "print(f\"F1-score (Class 1): {report_example.get('1', {}).get('f1-score', 0.0):.4f}\")\n",
        "print(f\"Accuracy: {report_example.get('accuracy', 0.0):.4f}\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "x3-QehJxbp0t",
        "4p9zp6bHfPOY",
        "lRrYccqLJKXi"
      ],
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}